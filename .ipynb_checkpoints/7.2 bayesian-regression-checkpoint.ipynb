{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Instructions\n",
    "1. Read through the code and descriptions, make sure you understand each line of code.\n",
    "2. There is functionality missing in the functions `show_posterior_surface` and  `draw_sample_models`.  Fill in the missing functionality.\n",
    "3. Answer the questions at the end of the worksheet.\n",
    "4. Come to class with your worksheet open and be able to paste your code or question answers into a poll.\n",
    "\n",
    "# Bayesian linear models\n",
    "\n",
    "This script provides a very basic introduction to making prediction with Bayesian linear models.\n",
    "Some of the functionality is missing and will need to be added before the entire script works.\n",
    "\n",
    "First we generate a data set.  Since this dataset is randomly generated, each time you run this cell you will get a new dataset to observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# If you are running this notebook locally  with ipython use:\n",
    "# %matplotlib notebook\n",
    "# and you will get beautiful rotatable graphs!\n",
    "\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "TRUE_MODEL_WEIGHTS = np.array([-1.0, -1.0])\n",
    "N = 5\n",
    "shape = (N, 2)\n",
    "sigma = 1.0\n",
    "vmax = 3\n",
    "\n",
    "w_1 = np.arange(-3, 3, 0.1)\n",
    "w_2 = np.arange(-3, 3, 0.1)\n",
    "\n",
    "\n",
    "def linear(w, x):\n",
    "    return np.dot(w, x.T)\n",
    "\n",
    "\n",
    "def rand_inputs(shape, vmax):\n",
    "    return vmax * np.random.rand(*shape)\n",
    "\n",
    "\n",
    "def rand_outputs(inputs, sigma):\n",
    "    true_output = linear(TRUE_MODEL_WEIGHTS, inputs)\n",
    "    noise = sigma * np.random.rand(*true_output.shape)\n",
    "    return true_output + noise\n",
    "\n",
    "\n",
    "xs = rand_inputs(shape, vmax)\n",
    "ys = rand_outputs(xs, sigma)\n",
    "\n",
    "\n",
    "def plot_data(title=None):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(xs[:, 0], xs[:, 1], ys)\n",
    "    ax.set_xlabel('$X_0$')\n",
    "    ax.set_ylabel('$X_1$')\n",
    "    ax.set_zlabel('$Y$')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "_ = plot_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Likelihood\n",
    "Given the data, we can now look at the likelihood of the data.\n",
    "Strictly speaking, this is a continuous function, but we will approximate this continuous function with a multidimensional grid of points over a plausible area of the model parameters.  In the figure below we plot the log-likelihood function since this is much nicer to visualize (as well as being more numerically stable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def loglikelihood(w, x, y, sigma):\n",
    "    return -sum((y - np.dot(w, x.T))**2) / sigma\n",
    "\n",
    "\n",
    "def standard_plot(Z, title=None):\n",
    "    plt.figure()\n",
    "    max_ind = np.unravel_index(Z.argmax(), Z.shape)\n",
    "    plt.contour(w_1, w_2, Z)\n",
    "    plt.scatter(w_1[max_ind[0]], w_2[max_ind[1]], label=\"Discretized max\")\n",
    "    plt.scatter(\n",
    "        TRUE_MODEL_WEIGHTS[0], TRUE_MODEL_WEIGHTS[1], label=\"Ground Truth\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"$w_1$\")\n",
    "    plt.ylabel(\"$w_2$\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "def show_log_likelihood_surface(x, y, sigma, estimates=None):\n",
    "    Z = np.zeros((w_1.size, w_2.size))\n",
    "\n",
    "    for (i, w_i) in enumerate(w_1):\n",
    "        for (j, w_j) in enumerate(w_2):\n",
    "            w_hat = np.array([w_i, w_j])\n",
    "            Z[i, j] = loglikelihood(w_hat, x, y, sigma)\n",
    "\n",
    "    standard_plot(Z, \"Log-Likelihood of the data\")\n",
    "    return Z\n",
    "\n",
    "\n",
    "log_likelihood = show_log_likelihood_surface(xs, ys, 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Prior\n",
    "As good Bayesians, we also provide a prior belief over our model parameters.  In this case we use a Normal distribution centered around zero to encode our belief that the weights are not large (i.e. far from zero).  Again we are plotting everything in log-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def show_log_prior_surface(sigma):\n",
    "    W_1, W_2 = np.meshgrid(w_1, w_2, sparse=False, indexing='ij')\n",
    "    W_1 = -(W_1**2)\n",
    "    W_2 = -(W_2**2)\n",
    "    Z = (W_1 + W_2) / sigma\n",
    "\n",
    "    standard_plot(Z, \"Log Prior of the model\")\n",
    "    return Z\n",
    "\n",
    "\n",
    "log_prior = show_log_prior_surface(1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Posterior\n",
    "Since our prior and likelihood plots have been in log-space we can get the posterior plot through simple addition (since addition in log-space corresponds to multiplication in normal space). However it is useful to also see the actual posterior.\n",
    "\n",
    "### Task\n",
    "1. Write code to transform the data into a proper posterior distribution.  Since the distribution is given in log-space, you will need to exponentiate the distribution and normalize it, so that it sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def show_log_posterior_surface(prior, likelihood):\n",
    "    Z = prior + likelihood\n",
    "    standard_plot(Z, \"Log-Posterior\")\n",
    "    return Z\n",
    "\n",
    "def show_posterior_surface(log_posterior):\n",
    "    # TODO: Let's turn this into a proper probability distribution!\n",
    "    # ...\n",
    "    # Do something other than this:\n",
    "    posterior = log_posterior\n",
    "    # ...\n",
    "    standard_plot(posterior, \"Normalized posterior\")\n",
    "    return posterior\n",
    "\n",
    "log_posterior = show_log_posterior_surface(log_prior, log_likelihood)\n",
    "posterior = show_posterior_surface(log_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Samples\n",
    "Now that we have a proper (discrete) distribution, we can draw samples from the distribution and show the predictions that those models make.\n",
    "\n",
    "##### Task\n",
    "1. Write code to draw `n` samples from the posterior distribution.  Here a single sample will return an entire model (this model is parameterized by the tuple `(w_1,w_2)`, so a single sample should return those two parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Draw some samples from the posterior!\n",
    "\n",
    "\n",
    "def draw_sample_models(n, posterior):\n",
    "    # Hint: the code below is not actually sampling from a posterior!\n",
    "    # TODO: Make it sample from the posterior!\n",
    "    return np.zeros((n,2))\n",
    "\n",
    "N_samples = 10\n",
    "ws = draw_sample_models(N_samples, posterior)\n",
    "flat_posterior = posterior.flatten()\n",
    "\n",
    "\n",
    "def show_model(ax, w):\n",
    "    X = np.array([[0.0, 0.0], [0.0, vmax], [vmax, 0.0], [vmax, vmax]])\n",
    "    X0 = X[:, 0].reshape((2, 2))\n",
    "    X1 = X[:, 1].reshape((2, 2))\n",
    "\n",
    "    Z = linear(w, X).reshape((2, 2))\n",
    "    ax.plot_surface(X0, X1, Z, alpha=1.0 / N_samples)\n",
    "\n",
    "\n",
    "ax = plot_data(\"Predictions from 10 approximate samples\")\n",
    "for ind in range(N_samples):\n",
    "    show_model(ax, ws[ind,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### `scikit.learn.BayesianRidge`\n",
    "In this particular case the model is simple enough that we don't need to use a discrete approximation and can instead find analytical solutions.  This has been coded up into `BayesianRidge` and is a part of scikit.learn.  When examining the results, notice that the error bars for the predictions increase as we move away from data that has already been seen.\n",
    "\n",
    "##### Task\n",
    "1. Find the online documentation for `BayesianRidge` to determine what model is being fitted, and what the parameters and hyperparameters are.\n",
    "\n",
    "(Note that the `return_std` option was only recently added to BayesianRidge (Dec 2016). If the code fails then please try updating your version of scikit.learn!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "N_grid = 30\n",
    "\n",
    "\n",
    "def error_bars(clf, title=None):\n",
    "    x = np.linspace(0, vmax, N_grid)\n",
    "    X0, X1 = np.meshgrid(x, x)\n",
    "    X = np.vstack((X0.flatten(), X1.flatten())).T\n",
    "    Y_hat, Y_std = clf.predict(X, return_std=True)\n",
    "    Y_hat = Y_hat.reshape((N_grid, N_grid))\n",
    "    Y_std = Y_std.reshape((N_grid, N_grid))\n",
    "\n",
    "    ax = plot_data()\n",
    "\n",
    "    ax.plot_surface(X0, X1, Y_hat + 1.96 * Y_std, alpha=0.1, label=\"Lower\")\n",
    "    ax.plot_surface(X0, X1, Y_hat, alpha=0.1, label=\"Mean\")\n",
    "    ax.plot_surface(X0, X1, Y_hat - 1.96 * Y_std, alpha=0.1, label=\"Upper\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "clf = BayesianRidge(compute_score=True, fit_intercept=False)\n",
    "clf.fit(xs, ys)\n",
    "error_bars(clf, \"Sci-kit's BayesianRidge predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Questions\n",
    "\n",
    "1. Given that we now have a number of models drawn from a posterior, how should we make a prediction for a single point?\n",
    "2. How do the models' predictions behave for points far away from observed data?\n",
    "3. What model does the maximum likelihood model correspond to?\n",
    "4. How does the behavior of the Bayesian linear model change as we observe more data?\n",
    "5. What other steps does the scikit learn BayesianRidge perform that we don't (list all of them)\n",
    "6. What hyperparameter(s) are in this model?\n",
    "7. Why don't we approximate more posterior distributions with a multi-dimensional grid of points?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (Ubuntu Linux)",
   "language": "python",
   "name": "python2-ubuntu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}